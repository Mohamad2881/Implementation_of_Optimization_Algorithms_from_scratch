# Implementation_of_Optimization_Algorithms_from_scratch
- Gradient Descent (Batach - Mini-Batch - Stochastic).
- Gradient Descent with Momentum.
- Nesterov Accelerated Gradient (NAG) Optimizer.
- Adaptive Gradient (ADAgrad) Optimizer.
- Root Mean Squared Propagation (RMSProp) Optimizer.
- Adaptive Moment Estimation (ADAM) Optimizer.
